{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "06eb7d62-a67d-4162-b173-bf49c3cedeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting logger_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile logger_config.py\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def setup_logger(run_path):\n",
    "    log_format = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    file_handler = logging.FileHandler(run_path / \"output.log\")\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(logging.Formatter(log_format))\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "a449109e-4b30-4424-8fb3-e17f8a037dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiment_setup_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiment_setup_snp.py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "MAIN_DIR = \"/gpfs/gibbs/pi/gerstein/tu54/imaging_project/expression-prediction/thyroid\"\n",
    "EXP_NAME = \"Thyroid-by-tile-NIC-CNN\"\n",
    "\n",
    "def get_run_folder(args):\n",
    "    args_str = f\"lr{args.learning_rate}-test_size{args.test_size}-batch_size{args.batch_size}-epochs{args.epochs}-column{args.snp_column}\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    return f\"run_{now_str}_{args_str}\"\n",
    "\n",
    "def create_path(parent_dir, child_dirs):\n",
    "    path = parent_dir\n",
    "    for child_dir in child_dirs:\n",
    "        path = path / child_dir\n",
    "        path.mkdir(exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def initialize_experiment(args):\n",
    "    current_path = create_path(Path(MAIN_DIR), [\"data\", EXP_NAME, get_run_folder(args)])\n",
    "    return current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "83b005f3-3bd1-432b-965d-dfdc0368c70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_setup_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_setup_snp.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from logger_config import logger\n",
    "\n",
    "def create_datasets(\n",
    "    csv_file_path: str,\n",
    "    image_dir_path: str,\n",
    "    current_path: str,\n",
    "    barcode_column: str,\n",
    "    seed: int,\n",
    "    test_size: float,\n",
    "    snp_column: int\n",
    "):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    data['genotype'] = data[barcode_column].apply(lambda x: 'AA' if x[snp_column] == '0' else 'AC' if x[snp_column] == '1' else 'CC')\n",
    "\n",
    "    image_file = [i.split(\"_\")[0] for i in os.listdir(image_dir_path)]\n",
    "    image_filenames_df = pd.DataFrame(image_file, columns=['image_file'])\n",
    "    image_filenames_df['Tissue Sample ID'] = image_filenames_df['image_file']\n",
    "\n",
    "    merged_data = pd.merge(image_filenames_df, data, left_on='Tissue Sample ID', right_on='Tissue Sample ID')\n",
    "    print(len(merged_data))\n",
    "\n",
    "    shuffled_data = merged_data.sample(frac=1, random_state=seed)\n",
    "    shuffled_data = shuffled_data.drop(columns=['Tissue Sample ID', 'Unnamed: 0', 'Tissue', 'Patient ID'])\n",
    "\n",
    "    logger.info(\"Final dataset (first few rows): \\n%s\", shuffled_data.head())\n",
    "\n",
    "    shuffled_data.to_csv(os.path.join(current_path, \"data_with_genotype.csv\"))\n",
    "\n",
    "    features = shuffled_data[['image_file', 'barcode']]\n",
    "    labels = shuffled_data['genotype']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_size, random_state=seed)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    y_train_onehot = to_categorical(y_train_encoded)\n",
    "    y_test_onehot = to_categorical(y_test_encoded)\n",
    "\n",
    "    class_names = ['AA', 'AC', 'CC']\n",
    "    logger.info(\"Class names: \\n%s\", class_names)\n",
    "\n",
    "    return (X_train, y_train_onehot), (X_test, y_test_onehot), class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "900baa29-bc60-4a55-bafc-c1f1a4b8a7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting augmentation_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile augmentation_snp.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from utils import selection\n",
    "from logger_config import logger\n",
    "\n",
    "IMG_DIR = \"/gpfs/gibbs/pi/gerstein/jrt62/imaging_project/expression-prediction/Thyroid/Thyroid-no-bounding-features-remove-bg\"\n",
    "\n",
    "def is_valid_patch(arr):\n",
    "    # Check if a patch is valid\n",
    "    return arr.shape[0] == 64 and arr.shape[1] == 64 and arr.shape[2] == 128 and np.isnan(arr[:,:,1]).sum() < 1000\n",
    "\n",
    "def augment_patch(arr, y, sum_x, sum_y, is_train_data):\n",
    "    # If the patch is valid, augment it and append it to sum_x and sum_y\n",
    "    if is_valid_patch(arr):\n",
    "        arr1 = np.nan_to_num(arr)\n",
    "        sum_x.append(arr1)\n",
    "        sum_y.append(y)\n",
    "        if is_train_data: \n",
    "            num = random.randint(1,7)\n",
    "            sum_x.append(selection(arr1, num))\n",
    "            sum_y.append(y)\n",
    "\n",
    "def process_slide(i, y_label, sum_x, sum_y, is_train_data):\n",
    "    # Process a whole slide\n",
    "    a = np.load(i + \"_features.npy\") # Load the compressed slide\n",
    "    b = np.swapaxes(a, 0, 2) # Swap axes\n",
    "    unit_list = []\n",
    "    \n",
    "    for j in range(128):\n",
    "        c = b[:,:,j][~np.isnan(b[:,:,j]).all(axis=1)] # Remove NaN rows\n",
    "        d = (c.T[~np.isnan(c.T).all(axis=1)]).T # Remove NaN columns\n",
    "        unit_list.append(d)\n",
    "        \n",
    "    e = np.array(unit_list)\n",
    "    f = np.swapaxes(e, 0, 2)\n",
    "    \n",
    "    f_len = f.shape[0]\n",
    "    f_width = f.shape[1]\n",
    "    f_len_int = f_len // 32\n",
    "    f_width_int = f_width // 32\n",
    "    \n",
    "    for k in range(f_len_int - 2):\n",
    "        for p in range(f_width_int - 2):\n",
    "            patch100 = f[k * 32:(k * 32 + 64), p * 32:(p * 32 + 64),:]\n",
    "            augment_patch(patch100, y_label, sum_x, sum_y, is_train_data)\n",
    "            \n",
    "        patch100 = f[k * 32:(k * 32 + 64), -65:-1,:]\n",
    "        augment_patch(patch100, y_label, sum_x, sum_y, is_train_data)\n",
    "\n",
    "    for p in range(f_width_int):\n",
    "        patch100 = f[-65:-1,p * 32:(p * 32 + 64),:]\n",
    "        augment_patch(patch100, y_label, sum_x, sum_y, is_train_data)\n",
    "    patch100 = f[-65:-1,-65:-1,:]\n",
    "    augment_patch(patch100, y_label, sum_x, sum_y, is_train_data)\n",
    "\n",
    "def process_all_slides(data, labels, is_train_data):\n",
    "    sum_x = []\n",
    "    sum_y = []\n",
    "\n",
    "    os.chdir(IMG_DIR)\n",
    "    if is_train_data:\n",
    "        logger.info(\"Processing all slides: training\")\n",
    "    else:\n",
    "        logger.info(\"Processing all slides: testing\")\n",
    "\n",
    "    for index, _ in enumerate(data.iterrows()):\n",
    "        # if is_train_data:\n",
    "        #     print(index)\n",
    "        # else:\n",
    "        #     print(_)\n",
    "        y_label = labels[index] # access label using integer index\n",
    "        image_file = data.iloc[index]['image_file']\n",
    "        process_slide(image_file, y_label, sum_x, sum_y, is_train_data)\n",
    "\n",
    "    logger.info(\"Finished processing.\")\n",
    "    return sum_x, sum_y\n",
    "\n",
    "def stack_data(sum_x, sum_y):\n",
    "    x = np.stack(sum_x)\n",
    "    y = np.array(sum_y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "5ad10847-ae95-40b1-b42f-58862563f75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_builder_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_builder_snp.py\n",
    "\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model(output_units: int, input_shape: tuple):\n",
    "    dropout = 0.3\n",
    "\n",
    "    # Input layer \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Conv Block 1\n",
    "    conv1 = layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(inputs)\n",
    "    pool1 = layers.MaxPooling2D()(conv1)\n",
    "    drop1 = layers.Dropout(dropout)(pool1)\n",
    "    \n",
    "    # Conv Block 2 \n",
    "    conv3 = layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(drop1)\n",
    "    pool2 = layers.MaxPooling2D()(conv3)\n",
    "    drop2 = layers.Dropout(dropout)(pool2)\n",
    "\n",
    "    # Conv Block 3 \n",
    "    conv5 = layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(drop2)\n",
    "    pool3 = layers.MaxPooling2D()(conv5)\n",
    "    drop3 = layers.Dropout(dropout)(pool3)\n",
    "\n",
    "    # Dense layers\n",
    "    flat = layers.Flatten()(drop3) \n",
    "    dense1 = layers.Dense(4096, activation='relu')(flat)\n",
    "    dense2 = layers.Dense(1024, activation='relu')(dense1)\n",
    "    dense3 = layers.Dense(128, activation='relu')(dense2)\n",
    "    dense4 = layers.Dense(16, activation='relu')(dense3)\n",
    "    outputs = layers.Dense(output_units, activation='softmax')(dense4)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "8f2939ed-6447-4b1e-9e4c-3e8a03a00b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting save_models_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile save_models_snp.py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pickle\n",
    "import numpy as np\n",
    "from logger_config import logger\n",
    "\n",
    "def save_model(model: tf.keras.Model,\n",
    "               history: tf.keras.callbacks.History,\n",
    "               current_path: str,\n",
    "               target_dir: str,\n",
    "               model_name: str,\n",
    "               y_pred: np.ndarray,\n",
    "               test_label: np.ndarray):\n",
    "    # Ensure target directory exists\n",
    "    target_dir_path = Path(current_path) / target_dir\n",
    "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".h5\"), \"model_name should end with '.h5'\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "    \n",
    "    # Save the model\n",
    "    # print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    logger.info(\"Saving model to: %s\", model_save_path)\n",
    "    model.save(model_save_path)\n",
    "    \n",
    "    # Save the training history\n",
    "    with open(target_dir_path / 'basic_history.pickle', 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    \n",
    "    # Save the prediction and true labels\n",
    "    np.save(target_dir_path / \"y_pred.npy\", y_pred)\n",
    "    np.save(target_dir_path / \"test_label.npy\", test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "5befb8cb-0da3-4db6-81da-fecaffbc2653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting plots_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile plots_snp.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "num_s=\"top250-8-tile-lr0.00001-optimized-r20-020123-multi-less-augumentation-more-shift\"\n",
    "\n",
    "def plot_learning_rate(a=0.00001, steps=20000, rate_decay=0.4):\n",
    "    y = []\n",
    "    x = range(steps)\n",
    "    for i in range(steps):\n",
    "        y.append(a * pow(rate_decay, i / 10000))\n",
    "\n",
    "    plt.figure(figsize=(8.5, 8))\n",
    "    plt.style.use(\"classic\")\n",
    "    plt.plot(x,y)\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(history, current_path):\n",
    "    plt.figure(figsize=(8.5, 8))\n",
    "    plt.style.use(\"classic\")\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(current_path / \"accuracy.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_categorical_crossentropy_loss(history, current_path):\n",
    "    plt.figure(figsize=(8.5, 8))\n",
    "    plt.style.use(\"classic\")\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('categorical crossentropy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(current_path / \"categorical_crossentropy_loss.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_f1_scores(y_true, y_pred, current_path, labels=None):\n",
    "    if len(y_true.shape) == 1:  # if y_true is integer encoded\n",
    "        y_true = to_categorical(y_true)\n",
    "    if len(y_pred.shape) == 1:  # if y_pred is integer encoded\n",
    "        y_pred = to_categorical(y_pred)\n",
    "\n",
    "    f1_scores = f1_score(y_true.argmax(axis=1), y_pred.argmax(axis=1), average=None)\n",
    "    plt.bar(range(len(f1_scores)), f1_scores)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Scores for Each Class')\n",
    "    if labels:\n",
    "        plt.xticks(range(len(f1_scores)), labels)\n",
    "    plt.savefig(current_path / \"f1_scores.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall(y_true, y_pred_proba, current_path, labels=None):\n",
    "    for i in range(y_pred_proba.shape[1]): # loop over each class\n",
    "        precision, recall, _ = precision_recall_curve(y_true[:, i], y_pred_proba[:, i])\n",
    "        label = f'Class {i}' if labels is None else labels[i]\n",
    "        plt.plot(recall, precision, lw=2, label=label)\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.savefig(current_path / \"precision_recall_curve.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def all_plots(history, y_pred, test_label, current_path, num_s, y_pred_proba=None):\n",
    "    plot_learning_rate()\n",
    "    plt.show()\n",
    "    \n",
    "    plot_accuracy(history, current_path)\n",
    "    \n",
    "    plot_categorical_crossentropy_loss(history, current_path)\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        plot_f1_scores(test_label, y_pred, current_path)\n",
    "        plot_precision_recall(test_label, y_pred_proba, current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "d87ddd05-1fc6-4338-b8dd-c801b7003544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_snp.py\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, losses\n",
    "import data_setup_snp, augmentation_snp, experiment_setup_snp, model_builder_snp, save_models_snp, plots_snp\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from matplotlib import pyplot as plt\n",
    "import argparse\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, average_precision_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "from logger_config import setup_logger, logger\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training script')\n",
    "parser.add_argument('--seed', type=int, default=20)\n",
    "parser.add_argument('--test_size', type=float, default=0.33)\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0004)\n",
    "parser.add_argument('--batch_size', type=int, default=10)\n",
    "parser.add_argument('--epochs', type=int, default=5)\n",
    "parser.add_argument('--snp_column', type=int, default=0)\n",
    "parser.add_argument('--ylabel', type=str, default='/gpfs/gibbs/pi/gerstein/tu54/imaging_project/barcodes_thyroid26.csv')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "num_s = \"top250-8-tile-lr0.00001-optimized-r20-020123-multi-less-augumentation-more-shift\"\n",
    "\n",
    "# Initialize experiment\n",
    "current_path = experiment_setup_snp.initialize_experiment(args)\n",
    "setup_logger(current_path)\n",
    "logger.info(tf.__version__)\n",
    "logger.info(tf.config.list_physical_devices('GPU'))\n",
    "logger.info('Experiment setup complete.')\n",
    "\n",
    "# Prepare training and testing data\n",
    "(train_data, train_labels), (test_data, test_labels), class_names = data_setup_snp.create_datasets(args.ylabel, \n",
    "                                                     '/gpfs/gibbs/pi/gerstein/jrt62/imaging_project/expression-prediction/Thyroid/Thyroid-no-bounding-features-remove-bg',\n",
    "                                                     current_path,\n",
    "                                                     'barcode',\n",
    "                                                     args.seed,\n",
    "                                                     args.test_size,\n",
    "                                                     args.snp_column)\n",
    "\n",
    "sum_x_train, sum_y_train = augmentation_snp.process_all_slides(train_data, train_labels, True)\n",
    "sum_x_test, sum_y_test = augmentation_snp.process_all_slides(test_data, test_labels, False)\n",
    "\n",
    "train_image, train_label = augmentation_snp.stack_data(sum_x_train, sum_y_train)\n",
    "test_image, test_label = augmentation_snp.stack_data(sum_x_test, sum_y_test)\n",
    "logger.info(\"Train image shape: %s\", train_image.shape)\n",
    "logger.info(\"Train label shape: %s\", train_label.shape)\n",
    "logger.info(\"Test image shape: %s\", test_image.shape)\n",
    "logger.info(\"Test label shape: %s\", test_label.shape)\n",
    "\n",
    "input_shape = train_image.shape[1:]\n",
    "output_units = train_label.shape[1]\n",
    "logger.info(\"Input shape: %s\", input_shape)\n",
    "logger.info(\"Output units: %s\", output_units)\n",
    "\n",
    "# checking class distribution before SMOTE\n",
    "class_distribution_train1 = np.sum(train_label, axis=0)\n",
    "class_distribution_test1 = np.sum(test_label, axis=0)\n",
    "logger.info(\"Training class distribution: %s\", class_distribution_train1)\n",
    "logger.info(\"Test class distribution: %s\", class_distribution_test1)\n",
    "plt.bar(range(len(class_distribution_train1)), class_distribution_train1)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Training Class Distribution')\n",
    "plt.savefig(current_path / \"class_distribution.png\", bbox_inches=\"tight\")\n",
    "\n",
    "# train_image = tf.cast(train_image, tf.float32)\n",
    "\n",
    "# # smote\n",
    "# smote = SMOTE(random_state=args.seed)\n",
    "# train_image, train_label = smote.fit_resample(train_image.numpy().reshape(-1, np.prod(input_shape)), train_label)\n",
    "# train_image = train_image.reshape(-1, *input_shape)\n",
    "\n",
    "# # checking class distribution after SMOTE\n",
    "# class_distribution_train = np.sum(train_label, axis=0)\n",
    "# class_distribution_test = np.sum(test_label, axis=0)\n",
    "# # print(\"Training class distribution:\", class_distribution_train)\n",
    "# # print(\"Test class distribution:\", class_distribution_test)\n",
    "# logger.info(\"Training class distribution: %s\", class_distribution_train)\n",
    "# logger.info(\"Test class distribution: %s\", class_distribution_test)\n",
    "# plt.bar(range(len(class_distribution_train)), class_distribution_train)\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Training Class Distribution')\n",
    "# plt.savefig(current_path / \"pre_class_distribution.png\", bbox_inches=\"tight\")\n",
    "\n",
    "# converting numpy array to a tensor\n",
    "train_image = tf.convert_to_tensor(train_image)\n",
    "train_label = tf.convert_to_tensor(train_label)\n",
    "\n",
    "# # one-hot encoded training labels\n",
    "y_train = np.argmax(train_labels, axis=1) # convert to label encoding if needed\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "logger.info(\"Class weight dict: \", class_weight_dict)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
    "\n",
    "model = model_builder_snp.build_model(output_units, input_shape)\n",
    "\n",
    "logger.info(model.summary())\n",
    "\n",
    "os.chdir(current_path)\n",
    "\n",
    "optimizer = Adam(learning_rate=args.learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "## attempting data balance with class weights\n",
    "history = model.fit(train_image, train_label,\n",
    "                    validation_data=(test_image, test_label),\n",
    "                    epochs=args.epochs,\n",
    "                    batch_size=args.batch_size,\n",
    "                    class_weight=class_weight_dict,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    loss, accuracy = model.evaluate(test_image, test_label)\n",
    "logger.info(\"Test loss: %s\\nTest accuracy: %s\", loss, accuracy)\n",
    "\n",
    "# Predicting the probabilities\n",
    "y_pred_proba = model.predict(test_image)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = np.argmax(test_label, axis=1)\n",
    "\n",
    "logger.info(\"\\n%s\", classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "for i in range(output_units):\n",
    "    precision, recall, _ = precision_recall_curve(test_label[:, i], y_pred_proba[:, i])\n",
    "    pr_auc = auc(recall, precision)\n",
    "    logger.info(\"PR AUC for class %s: %s\", i, pr_auc)\n",
    "\n",
    "save_models_snp.save_model(model=model,\n",
    "           history=history,\n",
    "           current_path=current_path,\n",
    "           target_dir=\"models\",\n",
    "           model_name=\"basic.h5\",\n",
    "           y_pred=y_pred_proba,\n",
    "           test_label=test_label)\n",
    "\n",
    "logger.info(\"Making plots\")\n",
    "plots_snp.all_plots(history, y_pred, test_label, current_path, num_s, y_pred_proba)\n",
    "logger.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "d9bcf0b0-8d14-4811-a7b7-aaab2d9e192d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Experiment setup complete.\n",
      "734\n",
      "Final dataset (first few rows): \n",
      "          image_file                     barcode genotype\n",
      "655   GTEX-ZYVF-1126  01210012111112001001121100       AA\n",
      "213   GTEX-S7PM-0826  20112111122112112021111101       CC\n",
      "601  GTEX-13N1W-0826  00100002110001200000211202       AA\n",
      "270   GTEX-S341-0226  22112010011110021121011000       CC\n",
      "171  GTEX-1EU9M-0626  21102200222122022020121112       CC\n",
      "Class names: \n",
      "['AA', 'AC', 'CC']\n",
      "Processing all slides: training\n",
      "Finished processing.\n",
      "Processing all slides: testing\n",
      "Finished processing.\n",
      "Train image shape: (1772, 64, 64, 128)\n",
      "Train label shape: (1772, 3)\n",
      "Test image shape: (478, 64, 64, 128)\n",
      "Test label shape: (478, 3)\n",
      "Input shape: (64, 64, 128)\n",
      "Output units: 3\n",
      "Training class distribution: [752. 704. 316.]\n",
      "Test class distribution: [188. 205.  85.]\n",
      "Class weight dict:  {0: 0.8265993265993266, 1: 0.8102310231023102, 2: 1.7985347985347986}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 64, 128)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 64, 64, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              67112960  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              4195328   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                2064      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,884,419\n",
      "Trainable params: 71,884,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "443/443 [==============================] - 10s 18ms/step - loss: 1.1342 - accuracy: 0.3251 - val_loss: 1.0986 - val_accuracy: 0.3766\n",
      "Epoch 2/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0879 - accuracy: 0.3352 - val_loss: 1.1031 - val_accuracy: 0.3326\n",
      "Epoch 3/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0800 - accuracy: 0.3894 - val_loss: 1.1007 - val_accuracy: 0.2615\n",
      "Epoch 4/100\n",
      "443/443 [==============================] - 6s 12ms/step - loss: 1.0716 - accuracy: 0.3900 - val_loss: 1.0886 - val_accuracy: 0.4205\n",
      "Epoch 5/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0684 - accuracy: 0.4266 - val_loss: 1.1179 - val_accuracy: 0.3724\n",
      "Epoch 6/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0618 - accuracy: 0.4407 - val_loss: 1.1145 - val_accuracy: 0.3912\n",
      "Epoch 7/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0576 - accuracy: 0.4391 - val_loss: 1.1240 - val_accuracy: 0.3828\n",
      "Epoch 8/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0542 - accuracy: 0.4458 - val_loss: 1.1267 - val_accuracy: 0.3870\n",
      "Epoch 9/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0526 - accuracy: 0.4464 - val_loss: 1.1202 - val_accuracy: 0.3870\n",
      "Epoch 10/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0546 - accuracy: 0.4362 - val_loss: 1.1055 - val_accuracy: 0.3891\n",
      "Epoch 11/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0818 - accuracy: 0.4424 - val_loss: 1.1011 - val_accuracy: 0.3849\n",
      "Epoch 12/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0495 - accuracy: 0.4458 - val_loss: 1.0992 - val_accuracy: 0.3870\n",
      "Epoch 13/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0457 - accuracy: 0.4458 - val_loss: 1.0965 - val_accuracy: 0.3870\n",
      "Epoch 14/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0497 - accuracy: 0.4436 - val_loss: 1.0967 - val_accuracy: 0.3891\n",
      "Epoch 15/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0451 - accuracy: 0.4464 - val_loss: 1.0987 - val_accuracy: 0.3870\n",
      "Epoch 16/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0453 - accuracy: 0.4413 - val_loss: 1.0969 - val_accuracy: 0.3912\n",
      "Epoch 17/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0451 - accuracy: 0.4453 - val_loss: 1.0975 - val_accuracy: 0.3891\n",
      "Epoch 18/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0468 - accuracy: 0.4447 - val_loss: 1.0998 - val_accuracy: 0.3870\n",
      "Epoch 19/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0438 - accuracy: 0.4453 - val_loss: 1.0997 - val_accuracy: 0.3891\n",
      "Epoch 20/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0540 - accuracy: 0.4441 - val_loss: 1.1014 - val_accuracy: 0.3870\n",
      "Epoch 21/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0477 - accuracy: 0.4470 - val_loss: 1.0939 - val_accuracy: 0.3933\n",
      "Epoch 22/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0445 - accuracy: 0.4464 - val_loss: 1.0947 - val_accuracy: 0.3891\n",
      "Epoch 23/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0615 - accuracy: 0.4470 - val_loss: 1.0919 - val_accuracy: 0.4059\n",
      "Epoch 24/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0412 - accuracy: 0.4667 - val_loss: 1.0835 - val_accuracy: 0.4205\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 1.0886 - accuracy: 0.4205\n",
      "Test loss: 1.0886187553405762\n",
      "Test accuracy: 0.42050209641456604\n",
      "15/15 [==============================] - 1s 32ms/step\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AA       0.41      0.87      0.55       188\n",
      "          AC       0.63      0.18      0.28       205\n",
      "          CC       0.06      0.01      0.02        85\n",
      "\n",
      "    accuracy                           0.42       478\n",
      "   macro avg       0.36      0.35      0.28       478\n",
      "weighted avg       0.44      0.42      0.34       478\n",
      "\n",
      "PR AUC for class 0: 0.6582601597714508\n",
      "PR AUC for class 1: 0.5348396886137968\n",
      "PR AUC for class 2: 0.1706247496151932\n",
      "Saving model to: /gpfs/gibbs/pi/gerstein/tu54/imaging_project/expression-prediction/thyroid/data/Thyroid-by-tile-NIC-CNN/run_2023-08-22-17-41-17_lr2e-05-test_size0.33-batch_size4-epochs100-column0/models/basic.h5\n",
      "Making plots\n",
      "Figure(640x480)\n",
      "Figure(850x800)\n",
      "Figure(680x640)\n",
      "Figure(680x640)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!python train_snp.py --seed 20 --test_size 0.33 --learning_rate 0.00002 --batch_size 4 --epochs 100 --snp_column 0 --ylabel '/gpfs/gibbs/pi/gerstein/tu54/imaging_project/barcodes_thyroid26.csv'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
