{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "a4f45103-1195-40e7-83e1-59756a2aedfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import optimizers, losses\n",
    "# # import data_setup_snp, augmentation_snp, experiment_setup_snp, model_builder_snp, save_models_snp, plots_snp\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from matplotlib import pyplot as plt\n",
    "# import argparse\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import roc_curve, average_precision_score, f1_score, precision_recall_curve, auc\n",
    "# from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "7280a779-dd32-49fa-94f0-4f090ffae2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir(\"/gpfs/gibbs/pi/gerstein/tu54/imaging_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "06eb7d62-a67d-4162-b173-bf49c3cedeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting logger_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile logger_config.py\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def setup_logger(run_path):\n",
    "    log_format = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    file_handler = logging.FileHandler(run_path / \"output.log\")\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(logging.Formatter(log_format))\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "a449109e-4b30-4424-8fb3-e17f8a037dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiment_setup_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiment_setup_snp.py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "MAIN_DIR = \"/gpfs/gibbs/pi/gerstein/tu54/imaging_project/expression-prediction/thyroid\"\n",
    "# EXP_NAME = \"Thyroid-by-tile-NIC-CNN-top250-8-tile-lr0.00001-optimized-r20-020123-multi-less-augumentation-more-shift\"\n",
    "EXP_NAME = \"Thyroid-by-tile-NIC-CNN\"\n",
    "\n",
    "# def get_run_folder():\n",
    "#     now_str = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "#     return f\"run_{now_str}\"\n",
    "\n",
    "def get_run_folder(args):\n",
    "    args_str = f\"lr{args.learning_rate}-test_size{args.test_size}-batch_size{args.batch_size}-epochs{args.epochs}-column{args.snp_column}\"\n",
    "    now_str = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    return f\"run_{now_str}_{args_str}\"\n",
    "\n",
    "def create_path(parent_dir, child_dirs):\n",
    "    path = parent_dir\n",
    "    for child_dir in child_dirs:\n",
    "        path = path / child_dir\n",
    "        path.mkdir(exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# def setup_logger(run_path):\n",
    "#     log_format = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "#     formatter = logging.Formatter(log_format)\n",
    "\n",
    "#     logger = logging.getLogger()\n",
    "#     logger.setLevel(logging.DEBUG)\n",
    "\n",
    "#     file_handler = logging.FileHandler(run_path / \"output.log\")\n",
    "#     file_handler.setLevel(logging.DEBUG)\n",
    "#     file_handler.setFormatter(formatter)\n",
    "#     logger.addHandler(file_handler)\n",
    "\n",
    "#     console_handler = logging.StreamHandler()\n",
    "#     console_handler.setLevel(logging.INFO)\n",
    "#     logger.addHandler(console_handler)\n",
    "    \n",
    "#     return logger\n",
    "\n",
    "# def initialize_experiment():\n",
    "#     current_path = create_path(Path(MAIN_DIR), [\"data\", EXP_NAME, get_run_folder()])\n",
    "#     logger = setup_logger(current_path)\n",
    "#     return current_path, logger\n",
    "\n",
    "def initialize_experiment(args):\n",
    "    current_path = create_path(Path(MAIN_DIR), [\"data\", EXP_NAME, get_run_folder(args)])\n",
    "    # logger = setup_logger(current_path)\n",
    "    return current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "3e622ae4-89ac-424a-8a0c-ced3f0006f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Tensorflow version: \", tf.__version__)\n",
    "# print(\"GPU: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "02c6051d-3064-4c01-ba3a-d6da80ae638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_path, logger = initialize_experiment()\n",
    "# print('Experiment setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "83b005f3-3bd1-432b-965d-dfdc0368c70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_setup_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_setup_snp.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from logger_config import logger\n",
    "\n",
    "def create_datasets(\n",
    "    csv_file_path: str,\n",
    "    image_dir_path: str,\n",
    "    current_path: str,\n",
    "    barcode_column: str,\n",
    "    seed: int,\n",
    "    test_size: float,\n",
    "    snp_column: int\n",
    "):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    # data = data.head(100) #work on small set first\n",
    "    # data['genotype'] = data[barcode_column].apply(lambda x: 'AA' if x[0] == '0' else 'AC' if x[0] == '1' else 'CC')\n",
    "    data['genotype'] = data[barcode_column].apply(lambda x: 'AA' if x[snp_column] == '0' else 'AC' if x[snp_column] == '1' else 'CC')\n",
    "\n",
    "    image_file = [i.split(\"_\")[0] for i in os.listdir(image_dir_path)]\n",
    "    image_filenames_df = pd.DataFrame(image_file, columns=['image_file'])\n",
    "    image_filenames_df['Tissue Sample ID'] = image_filenames_df['image_file']\n",
    "\n",
    "    merged_data = pd.merge(image_filenames_df, data, left_on='Tissue Sample ID', right_on='Tissue Sample ID')\n",
    "    print(len(merged_data))\n",
    "\n",
    "    shuffled_data = merged_data.sample(frac=1, random_state=seed)\n",
    "    shuffled_data = shuffled_data.drop(columns=['Tissue Sample ID', 'Unnamed: 0', 'Tissue', 'Patient ID'])\n",
    "\n",
    "    # print(\"Final dataset: \\n\", shuffled_data)\n",
    "    logger.info(\"Final dataset (first few rows): \\n%s\", shuffled_data.head())\n",
    "\n",
    "    shuffled_data.to_csv(os.path.join(current_path, \"data_with_genotype.csv\"))\n",
    "\n",
    "    features = shuffled_data[['image_file', 'barcode']]\n",
    "    labels = shuffled_data['genotype']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_size, random_state=seed)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    y_train_onehot = to_categorical(y_train_encoded)\n",
    "    y_test_onehot = to_categorical(y_test_encoded)\n",
    "\n",
    "    class_names = ['AA', 'AC', 'CC']\n",
    "    # print(\"Class names:\\n\", class_names)\n",
    "    logger.info(\"Class names: \\n%s\", class_names)\n",
    "\n",
    "    return (X_train, y_train_onehot), (X_test, y_test_onehot), class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "1fa8a1c1-aa16-4b98-a5e6-b3c16ea60491",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (train_data, train_labels), (test_data, test_labels), class_names = create_datasets(args.ylabel, \n",
    "#                                                      '/gpfs/gibbs/pi/gerstein/jrt62/imaging_project/expression-prediction/Thyroid/Thyroid-no-bounding-features-remove-bg',\n",
    "#                                                      current_path,\n",
    "#                                                      'barcode',\n",
    "#                                                      args.seed,\n",
    "#                                                      args.test_size,\n",
    "# args.snp_column)\n",
    "# (train_data, train_labels), (test_data, test_labels), class_names = create_datasets('/gpfs/gibbs/pi/gerstein/tu54/imaging_project/barcodes_thyroid26.csv', \n",
    "#                                                      '/gpfs/gibbs/pi/gerstein/jrt62/imaging_project/expression-prediction/Thyroid/Thyroid-no-bounding-features-remove-bg',\n",
    "#                                                      current_path,\n",
    "#                                                      'barcode',\n",
    "#                                                      20,\n",
    "#                                                      0.4,\n",
    "#                                                      0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "b5a844b8-e23b-4bee-97b7-87ee6092e41c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"Train labels: \", train_labels)\n",
    "# print(\"Test labels: \", test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "900baa29-bc60-4a55-bafc-c1f1a4b8a7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting augmentation_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile augmentation_snp.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from utils import selection\n",
    "from logger_config import logger\n",
    "\n",
    "IMG_DIR = \"/gpfs/gibbs/pi/gerstein/jrt62/imaging_project/expression-prediction/Thyroid/Thyroid-no-bounding-features-remove-bg\"\n",
    "\n",
    "def is_valid_patch(arr):\n",
    "    # Check if a patch is valid\n",
    "    return arr.shape[0] == 64 and arr.shape[1] == 64 and arr.shape[2] == 128 and np.isnan(arr[:,:,1]).sum() < 1000\n",
    "\n",
    "def augment_patch(arr, y, sum_x, sum_y, is_train_data):\n",
    "    # If the patch is valid, augment it and append it to sum_x and sum_y\n",
    "    if is_valid_patch(arr):\n",
    "        arr1 = np.nan_to_num(arr)\n",
    "        sum_x.append(arr1)\n",
    "        sum_y.append(y)\n",
    "        if is_train_data: \n",
    "            num = random.randint(1,7)\n",
    "            sum_x.append(selection(arr1, num))\n",
    "            sum_y.append(y)\n",
    "\n",
    "def process_slide(i, y_label, sum_x, sum_y, is_train_data):\n",
    "    # Process a whole slide\n",
    "    a = np.load(i + \"_features.npy\") # Load the compressed slide\n",
    "    b = np.swapaxes(a, 0, 2) # Swap axes\n",
    "    unit_list = []\n",
    "    \n",
    "    for j in range(128):\n",
    "        c = b[:,:,j][~np.isnan(b[:,:,j]).all(axis=1)] # Remove NaN rows\n",
    "        d = (c.T[~np.isnan(c.T).all(axis=1)]).T # Remove NaN columns\n",
    "        unit_list.append(d)\n",
    "        \n",
    "    e = np.array(unit_list)\n",
    "    f = np.swapaxes(e, 0, 2)\n",
    "    \n",
    "    f_len = f.shape[0]\n",
    "    f_width = f.shape[1]\n",
    "    f_len_int = f_len // 32\n",
    "    f_width_int = f_width // 32\n",
    "    \n",
    "    for k in range(f_len_int - 2):\n",
    "        for p in range(f_width_int - 2):\n",
    "            patch100 = f[k * 32:(k * 32 + 64), p * 32:(p * 32 + 64),:]\n",
    "            augment_patch(patch100, y_label, sum_x, sum_y, is_train_data)\n",
    "            \n",
    "        patch100 = f[k * 32:(k * 32 + 64), -65:-1,:]\n",
    "        augment_patch(patch100, y_label, sum_x, sum_y, is_train_data)\n",
    "\n",
    "    for p in range(f_width_int):\n",
    "        patch100 = f[-65:-1,p * 32:(p * 32 + 64),:]\n",
    "        augment_patch(patch100, y_label, sum_x, sum_y, is_train_data)\n",
    "    patch100 = f[-65:-1,-65:-1,:]\n",
    "    augment_patch(patch100, y_label, sum_x, sum_y, is_train_data)\n",
    "\n",
    "def process_all_slides(data, labels, is_train_data):\n",
    "    sum_x = []\n",
    "    sum_y = []\n",
    "\n",
    "    os.chdir(IMG_DIR)\n",
    "    if is_train_data:\n",
    "        logger.info(\"Processing all slides: training\")\n",
    "    else:\n",
    "        logger.info(\"Processing all slides: testing\")\n",
    "\n",
    "    for index, _ in enumerate(data.iterrows()):\n",
    "        # if is_train_data:\n",
    "        #     print(index)\n",
    "        # else:\n",
    "        #     print(_)\n",
    "        y_label = labels[index] # access label using integer index\n",
    "        image_file = data.iloc[index]['image_file']\n",
    "        process_slide(image_file, y_label, sum_x, sum_y, is_train_data)\n",
    "\n",
    "    logger.info(\"Finished processing.\")\n",
    "    return sum_x, sum_y\n",
    "\n",
    "def stack_data(sum_x, sum_y):\n",
    "    x = np.stack(sum_x)\n",
    "    y = np.array(sum_y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "70301e68-57c4-4d1d-986a-7ca4137c95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_x_train, sum_y_train = process_all_slides(train_data, train_labels, True)\n",
    "# sum_x_test, sum_y_test = process_all_slides(test_data, test_labels, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "b197742c-225f-40f4-a8cc-54d05444fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_image, train_label = stack_data(sum_x_train, sum_y_train)\n",
    "# test_image, test_label = stack_data(sum_x_test, sum_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "73bccf78-a660-4553-994a-61d6b4234385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Train image shape: \", train_image.shape)\n",
    "# print(\"Train label shape: \", train_label.shape)\n",
    "# print(\"Test image shape: \", test_image.shape)\n",
    "# print(\"Test label shape: \", test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "bccce73c-e847-4e92-8b37-3d665657098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.any(np.isnan(train_image)))\n",
    "# print(np.any(np.isinf(train_image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "5c59ebb8-d38e-40a3-b282-b9b55b20e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = train_image.shape[1:]\n",
    "# output_units = train_label.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "d544b1d9-0c42-4da0-84aa-9888bf397114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Input shape: \", input_shape)\n",
    "# print(\"Output units: \", output_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "47b617c6-1290-4cb6-8f43-6bfd61988561",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # checking class distribution before SMOTE\n",
    "# class_distribution_train1 = np.sum(train_label, axis=0)\n",
    "# class_distribution_test1 = np.sum(test_label, axis=0)\n",
    "# print(\"Training class distribution:\", class_distribution_train1)\n",
    "# print(\"Test class distribution:\", class_distribution_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "6e629a06-b94a-403a-a920-8e0dbeec4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # class distribution plot\n",
    "# plt.bar(range(len(class_distribution_train1)), class_distribution_train1)\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Training Class Distribution')\n",
    "# plt.savefig(current_path / \"class_distribution.png\", bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "86bc5ca1-5a33-42e6-8090-d803a4553c94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mean = tf.reduce_mean(train_image)\n",
    "# std = tf.math.reduce_std(train_image) \n",
    "# min_val = tf.reduce_min(train_image)\n",
    "# max_val = tf.reduce_max(train_image)\n",
    "\n",
    "# print(\"Mean:\", mean) \n",
    "# print(\"Standard Deviation:\", std)\n",
    "# print(\"Minimum:\", min_val)  \n",
    "# print(\"Maximum:\", max_val)\n",
    "\n",
    "# # statistics histogram\n",
    "# plt.hist(train_image.flatten(), bins=50)\n",
    "# plt.title(\"Pixel Value Distribution\")\n",
    "# plt.xlabel(\"Pixel Values\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "ba12b1f4-5d75-4cec-8674-2c0cbac728da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_image = tf.cast(train_image, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "e0da0820-1e3d-49e0-8c54-0fe784853540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # applying SMOTE after stacking data\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# # smote = SMOTE(random_state=args.seed)\n",
    "# smote = SMOTE(random_state=20)\n",
    "# train_image, train_label = smote.fit_resample(train_image.numpy().reshape(-1, np.prod(input_shape)), train_label) # Reshape train_image for SMOTE\n",
    "# train_image = train_image.reshape(-1, *input_shape) # Reshape back to original shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "4828ac45-585b-4eb9-a806-fffb8dd6ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking class distribution after SMOTE\n",
    "# class_distribution_train = np.sum(train_label, axis=0)\n",
    "# class_distribution_test = np.sum(test_label, axis=0)\n",
    "# print(\"Training class distribution:\", class_distribution_train)\n",
    "# print(\"Test class distribution:\", class_distribution_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "b0254dc1-93af-4e55-a3f6-06ec3bf3d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # class distribution plot\n",
    "# plt.bar(range(len(class_distribution_train)), class_distribution_train)\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Training Class Distribution')\n",
    "# plt.savefig(current_path / \"class_distribution.png\", bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "aa0992db-c659-43ac-9e72-9790f73be96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # converting numpy array to a tensor\n",
    "# train_image = tf.convert_to_tensor(train_image)\n",
    "# train_label = tf.convert_to_tensor(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "5753b1c6-cda4-4745-8de6-2d7e74ff5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # computing class weights with 'balanced' method\n",
    "# one-hot encoded training labels\n",
    "# y_train = np.argmax(train_labels, axis=1)\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "080efa2d-57f5-4e91-a17f-3628898ce9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert to dictionary format, for Keras\n",
    "# class_weight_dict = dict(enumerate(class_weights))\n",
    "# print(\"Class weight dict: \", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "5ad10847-ae95-40b1-b42f-58862563f75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_builder_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_builder_snp.py\n",
    "\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model(output_units: int, input_shape: tuple):\n",
    "    dropout = 0.3\n",
    "\n",
    "    # Input layer \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Conv Block 1\n",
    "    # conv1 = layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(inputs)\n",
    "    conv1 = layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(inputs)\n",
    "    pool1 = layers.MaxPooling2D()(conv1)\n",
    "    drop1 = layers.Dropout(dropout)(pool1)\n",
    "    \n",
    "    # Conv Block 2 \n",
    "    # conv3 = layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(drop1)\n",
    "    conv3 = layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(drop1)\n",
    "    pool2 = layers.MaxPooling2D()(conv3)\n",
    "    drop2 = layers.Dropout(dropout)(pool2)\n",
    "\n",
    "    # Conv Block 3 \n",
    "    # conv5 = layers.Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(drop2)\n",
    "    conv5 = layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(drop2)\n",
    "    pool3 = layers.MaxPooling2D()(conv5)\n",
    "    drop3 = layers.Dropout(dropout)(pool3)\n",
    "\n",
    "    # Dense layers\n",
    "    flat = layers.Flatten()(drop3) \n",
    "    # dense1 = layers.Dense(8192, activation='relu')(flat)\n",
    "    dense1 = layers.Dense(4096, activation='relu')(flat)\n",
    "    dense2 = layers.Dense(1024, activation='relu')(dense1)\n",
    "    # dense2 = layers.Dense(2048, activation='relu')(flat)\n",
    "    dense3 = layers.Dense(128, activation='relu')(dense2)\n",
    "    dense4 = layers.Dense(16, activation='relu')(dense3)\n",
    "    outputs = layers.Dense(output_units, activation='softmax')(dense4)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "2aa5b765-4daa-4627-b02e-046fb4d55341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(output_units, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "54fe6dab-fb41-462e-a880-cbcf81166bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "b785cf68-fa34-49b5-9ce5-9dca6ec0e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "66232e8a-e6ff-4df4-b5b0-66c4b846444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adam(learning_rate=args.learning_rate)\n",
    "# optimizer = Adam(learning_rate=0.00002)\n",
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "e5ebae59-b6fc-43c4-961e-ac113783a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "4929f105-fbca-41e8-bbd1-a15b9ce96104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=optimizer,\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "0a0c15c1-adb1-4df1-91a8-86f58c82f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "2f1b1eca-59d6-4656-8cf8-7734af87d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # no class weights\n",
    "# history = model.fit(train_image, train_label,\n",
    "#                     validation_data=(test_image, test_label),\n",
    "#                     epochs=100,\n",
    "#                     batch_size=4,\n",
    "#                     callbacks=[early_stopping],\n",
    "#                     verbose=1)\n",
    "\n",
    "# # history = model.fit(train_image, train_label,\n",
    "# #                     validation_data=(test_image, test_label),\n",
    "# #                     epochs=args.epochs,\n",
    "# #                     batch_size=args.batch_size,\n",
    "# #                     class_weight=class_weight_dict,\n",
    "# #                     callbacks=[early_stopping],\n",
    "# #                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "e1be1e8a-7431-4896-817d-a2af8145ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, accuracy = model.evaluate(test_image, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "d4028136-bc98-43ea-85a9-83da2887692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Test loss: {loss}\\nTest accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "dec061e2-58ac-4a3b-8a42-33ec4dbc7e04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# redicting the probabilities\n",
    "# y_pred_proba = model.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "7d9e9a1f-d215-44f0-a8d5-cec9e2a362fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "# y_true = np.argmax(test_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "ac3c6407-01e0-4cf4-877a-bdae7baa44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = f1_score(y_true, y_pred, average='micro') # 'macro' for unbalanced classes\n",
    "# print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "4de77d9e-aee8-4879-9634-3122ecaf10bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # alculate PR AUC for each class\n",
    "# for i in range(output_units):\n",
    "#     precision, recall, _ = precision_recall_curve(test_label[:, i], y_pred_proba[:, i])\n",
    "#     pr_auc = auc(recall, precision)\n",
    "#     print(f\"PR AUC for class {i}: {pr_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "8f2939ed-6447-4b1e-9e4c-3e8a03a00b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting save_models_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile save_models_snp.py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pickle\n",
    "import numpy as np\n",
    "from logger_config import logger\n",
    "\n",
    "def save_model(model: tf.keras.Model,\n",
    "               history: tf.keras.callbacks.History,\n",
    "               current_path: str,\n",
    "               target_dir: str,\n",
    "               model_name: str,\n",
    "               y_pred: np.ndarray,\n",
    "               test_label: np.ndarray):\n",
    "    # Ensure target directory exists\n",
    "    target_dir_path = Path(current_path) / target_dir\n",
    "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".h5\"), \"model_name should end with '.h5'\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "    \n",
    "    # Save the model\n",
    "    # print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    logger.info(\"Saving model to: %s\", model_save_path)\n",
    "    model.save(model_save_path)\n",
    "    \n",
    "    # Save the training history\n",
    "    with open(target_dir_path / 'basic_history.pickle', 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    \n",
    "    # Save the prediction and true labels\n",
    "    np.save(target_dir_path / \"y_pred.npy\", y_pred)\n",
    "    np.save(target_dir_path / \"test_label.npy\", test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "0e65b2c2-ea28-4b56-9629-db057c23a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model=model,\n",
    "#            history=history,\n",
    "#            current_path=current_path,\n",
    "#            target_dir=\"models\",\n",
    "#            model_name=\"basic.h5\",\n",
    "#            y_pred=y_pred_proba,\n",
    "#            test_label=test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "5befb8cb-0da3-4db6-81da-fecaffbc2653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting plots_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile plots_snp.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "num_s=\"top250-8-tile-lr0.00001-optimized-r20-020123-multi-less-augumentation-more-shift\"\n",
    "\n",
    "def plot_learning_rate(a=0.00001, steps=20000, rate_decay=0.4):\n",
    "    y = []\n",
    "    x = range(steps)\n",
    "    for i in range(steps):\n",
    "        y.append(a * pow(rate_decay, i / 10000))\n",
    "\n",
    "    plt.figure(figsize=(8.5, 8))\n",
    "    plt.style.use(\"classic\")\n",
    "    plt.plot(x,y)\n",
    "    # plt.savefig(current_path / \"learning_rate.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(history, current_path):\n",
    "    plt.figure(figsize=(8.5, 8))\n",
    "    plt.style.use(\"classic\")\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(current_path / \"accuracy.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_categorical_crossentropy_loss(history, current_path):\n",
    "    plt.figure(figsize=(8.5, 8))\n",
    "    plt.style.use(\"classic\")\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('categorical crossentropy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(current_path / \"categorical_crossentropy_loss.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_f1_scores(y_true, y_pred, current_path, labels=None):\n",
    "    if len(y_true.shape) == 1:  # if y_true is integer encoded\n",
    "        y_true = to_categorical(y_true)\n",
    "    if len(y_pred.shape) == 1:  # if y_pred is integer encoded\n",
    "        y_pred = to_categorical(y_pred)\n",
    "\n",
    "    f1_scores = f1_score(y_true.argmax(axis=1), y_pred.argmax(axis=1), average=None)\n",
    "    plt.bar(range(len(f1_scores)), f1_scores)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Scores for Each Class')\n",
    "    if labels:\n",
    "        plt.xticks(range(len(f1_scores)), labels)\n",
    "    plt.savefig(current_path / \"f1_scores.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall(y_true, y_pred_proba, current_path, labels=None):\n",
    "    for i in range(y_pred_proba.shape[1]): # loop over each class\n",
    "        precision, recall, _ = precision_recall_curve(y_true[:, i], y_pred_proba[:, i])\n",
    "        label = f'Class {i}' if labels is None else labels[i]\n",
    "        plt.plot(recall, precision, lw=2, label=label)\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.savefig(current_path / \"precision_recall_curve.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def all_plots(history, y_pred, test_label, current_path, num_s, y_pred_proba=None):\n",
    "    plot_learning_rate()\n",
    "    plt.show()\n",
    "    \n",
    "    plot_accuracy(history, current_path)\n",
    "    \n",
    "    plot_categorical_crossentropy_loss(history, current_path)\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        plot_f1_scores(test_label, y_pred, current_path)\n",
    "        plot_precision_recall(test_label, y_pred_proba, current_path)\n",
    "# all_plots(history, y_pred, test_label, current_path, num_s, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "d87ddd05-1fc6-4338-b8dd-c801b7003544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_snp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_snp.py\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, losses\n",
    "import data_setup_snp, augmentation_snp, experiment_setup_snp, model_builder_snp, save_models_snp, plots_snp\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from matplotlib import pyplot as plt\n",
    "import argparse\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, average_precision_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "from logger_config import setup_logger, logger\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training script')\n",
    "parser.add_argument('--seed', type=int, default=20)\n",
    "parser.add_argument('--test_size', type=float, default=0.33)\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0004)\n",
    "parser.add_argument('--batch_size', type=int, default=10)\n",
    "parser.add_argument('--epochs', type=int, default=5)\n",
    "parser.add_argument('--snp_column', type=int, default=0)\n",
    "parser.add_argument('--ylabel', type=str, default='/gpfs/gibbs/pi/gerstein/tu54/imaging_project/barcodes_thyroid26.csv')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "num_s = \"top250-8-tile-lr0.00001-optimized-r20-020123-multi-less-augumentation-more-shift\"\n",
    "\n",
    "# Initialize experiment\n",
    "current_path = experiment_setup_snp.initialize_experiment(args)\n",
    "setup_logger(current_path)\n",
    "logger.info(tf.__version__)\n",
    "logger.info(tf.config.list_physical_devices('GPU'))\n",
    "logger.info('Experiment setup complete.')\n",
    " \n",
    "# Prepare training and testing data\n",
    "(train_data, train_labels), (test_data, test_labels), class_names = data_setup_snp.create_datasets(args.ylabel, \n",
    "                                                     '/gpfs/gibbs/pi/gerstein/jrt62/imaging_project/expression-prediction/Thyroid/Thyroid-no-bounding-features-remove-bg',\n",
    "                                                     current_path,\n",
    "                                                     'barcode',\n",
    "                                                     args.seed,\n",
    "                                                     args.test_size,\n",
    "                                                     args.snp_column)\n",
    "\n",
    "# print(\"Train labels: \", train_labels)\n",
    "# print(\"Test labels: \", test_labels)\n",
    "\n",
    "sum_x_train, sum_y_train = augmentation_snp.process_all_slides(train_data, train_labels, True)\n",
    "sum_x_test, sum_y_test = augmentation_snp.process_all_slides(test_data, test_labels, False)\n",
    "# print(len(sum_x_train))\n",
    "# print(len(sum_x_test))\n",
    "\n",
    "train_image, train_label = augmentation_snp.stack_data(sum_x_train, sum_y_train)\n",
    "test_image, test_label = augmentation_snp.stack_data(sum_x_test, sum_y_test)\n",
    "# print(\"Train image shape: \", train_image.shape)\n",
    "# print(\"Train label shape: \", train_label.shape)\n",
    "# print(\"Test image shape: \", test_image.shape)\n",
    "# print(\"Test label shape: \", test_label.shape)\n",
    "logger.info(\"Train image shape: %s\", train_image.shape)\n",
    "logger.info(\"Train label shape: %s\", train_label.shape)\n",
    "logger.info(\"Test image shape: %s\", test_image.shape)\n",
    "logger.info(\"Test label shape: %s\", test_label.shape)\n",
    "\n",
    "input_shape = train_image.shape[1:]\n",
    "output_units = train_label.shape[1]\n",
    "# print(\"Input shape: \", input_shape)\n",
    "# print(\"Output units: \", output_units)\n",
    "logger.info(\"Input shape: %s\", input_shape)\n",
    "logger.info(\"Output units: %s\", output_units)\n",
    "\n",
    "# checking class distribution before SMOTE\n",
    "class_distribution_train1 = np.sum(train_label, axis=0)\n",
    "class_distribution_test1 = np.sum(test_label, axis=0)\n",
    "# print(\"Training class distribution:\", class_distribution_train1)\n",
    "# print(\"Test class distribution:\", class_distribution_test1)\n",
    "logger.info(\"Training class distribution: %s\", class_distribution_train1)\n",
    "logger.info(\"Test class distribution: %s\", class_distribution_test1)\n",
    "plt.bar(range(len(class_distribution_train1)), class_distribution_train1)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Training Class Distribution')\n",
    "plt.savefig(current_path / \"class_distribution.png\", bbox_inches=\"tight\")\n",
    "\n",
    "# train_image = tf.cast(train_image, tf.float32)\n",
    "\n",
    "# # smote\n",
    "# smote = SMOTE(random_state=args.seed)\n",
    "# train_image, train_label = smote.fit_resample(train_image.numpy().reshape(-1, np.prod(input_shape)), train_label)\n",
    "# train_image = train_image.reshape(-1, *input_shape)\n",
    "\n",
    "# # checking class distribution after SMOTE\n",
    "# class_distribution_train = np.sum(train_label, axis=0)\n",
    "# class_distribution_test = np.sum(test_label, axis=0)\n",
    "# # print(\"Training class distribution:\", class_distribution_train)\n",
    "# # print(\"Test class distribution:\", class_distribution_test)\n",
    "# logger.info(\"Training class distribution: %s\", class_distribution_train)\n",
    "# logger.info(\"Test class distribution: %s\", class_distribution_test)\n",
    "# plt.bar(range(len(class_distribution_train)), class_distribution_train)\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Training Class Distribution')\n",
    "# plt.savefig(current_path / \"pre_class_distribution.png\", bbox_inches=\"tight\")\n",
    "\n",
    "# converting numpy array to a tensor\n",
    "train_image = tf.convert_to_tensor(train_image)\n",
    "train_label = tf.convert_to_tensor(train_label)\n",
    "\n",
    "# # one-hot encoded training labels\n",
    "y_train = np.argmax(train_labels, axis=1) # convert to label encoding if needed\n",
    "\n",
    "# computing class weights with 'balanced' method\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "# convert to dictionary format, for Keras\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"Class weight dict: \", class_weight_dict)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
    "\n",
    "model = model_builder_snp.build_model(output_units, input_shape)\n",
    "\n",
    "logger.info(model.summary())\n",
    "\n",
    "os.chdir(current_path)\n",
    "\n",
    "optimizer = Adam(learning_rate=args.learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "## attempting data balance with class weights\n",
    "history = model.fit(train_image, train_label,\n",
    "                    validation_data=(test_image, test_label),\n",
    "                    epochs=args.epochs,\n",
    "                    batch_size=args.batch_size,\n",
    "                    class_weight=class_weight_dict,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)\n",
    "\n",
    "# history = model.fit(train_image, train_label,\n",
    "#                     validation_data=(test_image, test_label),\n",
    "#                     epochs=args.epochs,\n",
    "#                     batch_size=args.batch_size,\n",
    "#                     callbacks=[early_stopping],\n",
    "#                     verbose=1)\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    loss, accuracy = model.evaluate(test_image, test_label)\n",
    "# print(f\"Test loss: {loss}\\nTest accuracy: {accuracy}\")\n",
    "logger.info(\"Test loss: %s\\nTest accuracy: %s\", loss, accuracy)\n",
    "\n",
    "# Predicting the probabilities\n",
    "y_pred_proba = model.predict(test_image)\n",
    "\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = np.argmax(test_label, axis=1)\n",
    "\n",
    "# f1 = f1_score(y_true, y_pred, average='macro') # 'macro' for unbalanced classes\n",
    "# print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "logger.info(\"\\n%s\", classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "# Calculate PR AUC for each class\n",
    "# for i in range(output_units):\n",
    "#     precision, recall, _ = precision_recall_curve(test_label[:, i], y_pred_proba[:, i])\n",
    "#     pr_auc = auc(recall, precision)\n",
    "#     print(f\"PR AUC for class {i}: {pr_auc}\")\n",
    "for i in range(output_units):\n",
    "    precision, recall, _ = precision_recall_curve(test_label[:, i], y_pred_proba[:, i])\n",
    "    pr_auc = auc(recall, precision)\n",
    "    logger.info(\"PR AUC for class %s: %s\", i, pr_auc)\n",
    "\n",
    "save_models_snp.save_model(model=model,\n",
    "           history=history,\n",
    "           current_path=current_path,\n",
    "           target_dir=\"models\",\n",
    "           model_name=\"basic.h5\",\n",
    "           y_pred=y_pred_proba,\n",
    "           test_label=test_label)\n",
    "\n",
    "logger.info(\"Making plots\")\n",
    "plots_snp.all_plots(history, y_pred, test_label, current_path, num_s, y_pred_proba)\n",
    "logger.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "d9bcf0b0-8d14-4811-a7b7-aaab2d9e192d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Experiment setup complete.\n",
      "734\n",
      "Final dataset (first few rows): \n",
      "          image_file                     barcode genotype\n",
      "655   GTEX-ZYVF-1126  01210012111112001001121100       AA\n",
      "213   GTEX-S7PM-0826  20112111122112112021111101       CC\n",
      "601  GTEX-13N1W-0826  00100002110001200000211202       AA\n",
      "270   GTEX-S341-0226  22112010011110021121011000       CC\n",
      "171  GTEX-1EU9M-0626  21102200222122022020121112       CC\n",
      "Class names: \n",
      "['AA', 'AC', 'CC']\n",
      "Processing all slides: training\n",
      "Finished processing.\n",
      "Processing all slides: testing\n",
      "Finished processing.\n",
      "Train image shape: (1772, 64, 64, 128)\n",
      "Train label shape: (1772, 3)\n",
      "Test image shape: (478, 64, 64, 128)\n",
      "Test label shape: (478, 3)\n",
      "Input shape: (64, 64, 128)\n",
      "Output units: 3\n",
      "Training class distribution: [752. 704. 316.]\n",
      "Test class distribution: [188. 205.  85.]\n",
      "Class weight dict:  {0: 0.8265993265993266, 1: 0.8102310231023102, 2: 1.7985347985347986}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 64, 128)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 64, 64, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              67112960  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              4195328   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                2064      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,884,419\n",
      "Trainable params: 71,884,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "443/443 [==============================] - 10s 18ms/step - loss: 1.1342 - accuracy: 0.3251 - val_loss: 1.0986 - val_accuracy: 0.3766\n",
      "Epoch 2/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0879 - accuracy: 0.3352 - val_loss: 1.1031 - val_accuracy: 0.3326\n",
      "Epoch 3/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0800 - accuracy: 0.3894 - val_loss: 1.1007 - val_accuracy: 0.2615\n",
      "Epoch 4/100\n",
      "443/443 [==============================] - 6s 12ms/step - loss: 1.0716 - accuracy: 0.3900 - val_loss: 1.0886 - val_accuracy: 0.4205\n",
      "Epoch 5/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0684 - accuracy: 0.4266 - val_loss: 1.1179 - val_accuracy: 0.3724\n",
      "Epoch 6/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0618 - accuracy: 0.4407 - val_loss: 1.1145 - val_accuracy: 0.3912\n",
      "Epoch 7/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0576 - accuracy: 0.4391 - val_loss: 1.1240 - val_accuracy: 0.3828\n",
      "Epoch 8/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0542 - accuracy: 0.4458 - val_loss: 1.1267 - val_accuracy: 0.3870\n",
      "Epoch 9/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0526 - accuracy: 0.4464 - val_loss: 1.1202 - val_accuracy: 0.3870\n",
      "Epoch 10/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0546 - accuracy: 0.4362 - val_loss: 1.1055 - val_accuracy: 0.3891\n",
      "Epoch 11/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0818 - accuracy: 0.4424 - val_loss: 1.1011 - val_accuracy: 0.3849\n",
      "Epoch 12/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0495 - accuracy: 0.4458 - val_loss: 1.0992 - val_accuracy: 0.3870\n",
      "Epoch 13/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0457 - accuracy: 0.4458 - val_loss: 1.0965 - val_accuracy: 0.3870\n",
      "Epoch 14/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0497 - accuracy: 0.4436 - val_loss: 1.0967 - val_accuracy: 0.3891\n",
      "Epoch 15/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0451 - accuracy: 0.4464 - val_loss: 1.0987 - val_accuracy: 0.3870\n",
      "Epoch 16/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0453 - accuracy: 0.4413 - val_loss: 1.0969 - val_accuracy: 0.3912\n",
      "Epoch 17/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0451 - accuracy: 0.4453 - val_loss: 1.0975 - val_accuracy: 0.3891\n",
      "Epoch 18/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0468 - accuracy: 0.4447 - val_loss: 1.0998 - val_accuracy: 0.3870\n",
      "Epoch 19/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0438 - accuracy: 0.4453 - val_loss: 1.0997 - val_accuracy: 0.3891\n",
      "Epoch 20/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0540 - accuracy: 0.4441 - val_loss: 1.1014 - val_accuracy: 0.3870\n",
      "Epoch 21/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0477 - accuracy: 0.4470 - val_loss: 1.0939 - val_accuracy: 0.3933\n",
      "Epoch 22/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0445 - accuracy: 0.4464 - val_loss: 1.0947 - val_accuracy: 0.3891\n",
      "Epoch 23/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0615 - accuracy: 0.4470 - val_loss: 1.0919 - val_accuracy: 0.4059\n",
      "Epoch 24/100\n",
      "443/443 [==============================] - 5s 12ms/step - loss: 1.0412 - accuracy: 0.4667 - val_loss: 1.0835 - val_accuracy: 0.4205\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 1.0886 - accuracy: 0.4205\n",
      "Test loss: 1.0886187553405762\n",
      "Test accuracy: 0.42050209641456604\n",
      "15/15 [==============================] - 1s 32ms/step\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AA       0.41      0.87      0.55       188\n",
      "          AC       0.63      0.18      0.28       205\n",
      "          CC       0.06      0.01      0.02        85\n",
      "\n",
      "    accuracy                           0.42       478\n",
      "   macro avg       0.36      0.35      0.28       478\n",
      "weighted avg       0.44      0.42      0.34       478\n",
      "\n",
      "PR AUC for class 0: 0.6582601597714508\n",
      "PR AUC for class 1: 0.5348396886137968\n",
      "PR AUC for class 2: 0.1706247496151932\n",
      "Saving model to: /gpfs/gibbs/pi/gerstein/tu54/imaging_project/expression-prediction/thyroid/data/Thyroid-by-tile-NIC-CNN/run_2023-08-22-17-41-17_lr2e-05-test_size0.33-batch_size4-epochs100-column0/models/basic.h5\n",
      "Making plots\n",
      "Figure(640x480)\n",
      "Figure(850x800)\n",
      "Figure(680x640)\n",
      "Figure(680x640)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!python train_snp.py --seed 20 --test_size 0.33 --learning_rate 0.00002 --batch_size 4 --epochs 100 --snp_column 0 --ylabel '/gpfs/gibbs/pi/gerstein/tu54/imaging_project/barcodes_thyroid26.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "f3b81b98-601a-4c89-a03c-fd5657ebac4e",
   "metadata": {
    "autoscroll": false,
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Previous models\n",
    "\n",
    "# # def build_model(output_units: int, input_shape: tuple):\n",
    "    \n",
    "# #     dropout = 0.2\n",
    "# #     #l2_reg = 0.001 #regularization factor\n",
    "    \n",
    "# #     # Input layer \n",
    "# #     inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "# #     # Conv Block 1\n",
    "# #     conv1 = layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(inputs)\n",
    "# #     #conv2 = layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(conv1)\n",
    "# #     pool1 = layers.MaxPooling2D()(conv1)\n",
    "# #     #bn1 = layers.BatchNormalization()(pool1)\n",
    "# #     drop1 = layers.Dropout(dropout)(pool1)#(bn1)\n",
    "    \n",
    "# #     # Conv Block 2 \n",
    "# #     conv3 = layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(drop1)\n",
    "# #     #conv4 = layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(conv3)\n",
    "# #     pool2 = layers.MaxPooling2D()(conv3)\n",
    "# #     #bn2 = layers.BatchNormalization()(pool2)\n",
    "# #     drop2 = layers.Dropout(dropout)(pool2)\n",
    "\n",
    "# #     # Conv Block 3 \n",
    "# #     conv5 = layers.Conv2D(filters=512, kernel_size=3, padding='same', activation='relu')(drop2)\n",
    "# #     #conv6 = layers.Conv2D(filters=512, kernel_size=3, padding='same')(conv5)\n",
    "# #     #conv6 = layers.ReLU()(conv6)#LeakyReLU()(conv6) # from relu to leakyrelu\n",
    "# #     pool3 = layers.MaxPooling2D()(conv5)\n",
    "# #     #bn3 = layers.BatchNormalization()(pool3)\n",
    "# #     drop3 = layers.Dropout(dropout)(pool3)\n",
    "\n",
    "# #     # Dense layers\n",
    "# #     flat = layers.Flatten()(drop3) \n",
    "# #     dense1 = layers.Dense(8192, activation='relu')(flat)\n",
    "# #     dense2 = layers.Dense(1024, activation='relu')(dense1)\n",
    "# #     dense3 = layers.Dense(128, activation='relu')(dense2) # added intermediate dense layer\n",
    "# #     dense4 = layers.Dense(16, activation='relu')(dense3)\n",
    "# #     outputs = layers.Dense(output_units, activation='softmax')(dense4)\n",
    "# #     model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# #     return model\n",
    "\n",
    "#     '''\n",
    "#     #######model 1\n",
    "#     dropout = 0.2\n",
    "#     #l2_reg = 0.001 #regularization factor\n",
    "    \n",
    "#     # Input layer \n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "#     # Conv Block 1\n",
    "#     conv1 = layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)\n",
    "#     conv2 = layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')(conv1)\n",
    "#     pool1 = layers.MaxPooling2D()(conv2)\n",
    "#     bn1 = layers.BatchNormalization()(pool1)\n",
    "#     drop1 = layers.Dropout(dropout)(bn1)\n",
    "    \n",
    "#     # Conv Block 2 \n",
    "#     conv3 = layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(drop1)\n",
    "#     conv4 = layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(conv3)\n",
    "#     pool2 = layers.MaxPooling2D()(conv4)\n",
    "#     bn2 = layers.BatchNormalization()(pool2)\n",
    "#     drop2 = layers.Dropout(dropout)(bn2)\n",
    "\n",
    "#     # Conv Block 3 \n",
    "#     conv5 = layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(drop2)\n",
    "#     conv6 = layers.Conv2D(filters=256, kernel_size=3, padding='same')(conv5)\n",
    "#     conv6 = layers.LeakyReLU()(conv6) # from relu to leakyrelu\n",
    "#     pool3 = layers.MaxPooling2D()(conv6)\n",
    "#     bn3 = layers.BatchNormalization()(pool3)\n",
    "#     drop3 = layers.Dropout(dropout)(bn3)\n",
    "\n",
    "#     # Dense layers\n",
    "#     flat = layers.Flatten()(drop3) \n",
    "#     dense1 = layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(flat)\n",
    "#     dense2 = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(dense1)\n",
    "#     dense3 = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(dense2) # added intermediate dense layer\n",
    "#     outputs = layers.Dense(output_units, activation='softmax')(dense3)\n",
    "#     model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "#     #####model 2\n",
    "#     l2_reg = 0.0002\n",
    "    \n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "#     # Conv Block 1\n",
    "#     x = layers.Conv2D(filters=32, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(l2_reg))(inputs)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Conv2D(filters=32, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.MaxPooling2D()(x)\n",
    "#     x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "#     # Conv Block 2 \n",
    "#     x = layers.Conv2D(filters=64, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Conv2D(filters=64, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "#     x = layers.MaxPooling2D()(x)\n",
    "#     x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "#     # Conv Block 3 \n",
    "#     x = layers.Conv2D(filters=128, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "#     x = layers.LeakyReLU()(x)\n",
    "#     x = layers.MaxPooling2D()(x)\n",
    "#     x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "#     # Dense layers\n",
    "#     x = layers.Flatten()(x)\n",
    "#     x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "#     x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "#     x = layers.Dropout(0.2)(x)\n",
    "#     outputs = layers.Dense(output_units, activation='softmax')(x)\n",
    "    \n",
    "#     model = tf.keras.Model(inputs, outputs)\n",
    "#     '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "942fce1e-8075-4ea0-946c-5bcb85cb5351",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve, auc, average_precision_score\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# # For PRROC:\n",
    "\n",
    "# # Use prediction as probability\n",
    "# y_pred_proba = y_pred  \n",
    "# y_test_binary = np.argmax(test_label, axis=-1) # Convert one-hot labels to binary\n",
    "\n",
    "# # Assume y_true is multiclass label encoding \n",
    "# # e.g. [0, 1, 2, 1, 0, 2, ...] for 3 classes\n",
    "\n",
    "# num_classes = 3\n",
    "\n",
    "# # Convert to one-vs-all binary \n",
    "# y_true_binary = np.zeros((len(train_label), num_classes))\n",
    "# for i, label in enumerate(train_label):\n",
    "#   y_true_binary[i, label] = 1\n",
    "\n",
    "# # Calculate ROC curve for each class\n",
    "# precision, recall, thresholds = roc_curve(y_true_binary[:,0], y_pred[:,0])  \n",
    "# # Class 0\n",
    "# precision, recall, thresholds = roc_curve(y_true_binary[:,1], y_pred[:,1])\n",
    "# # Class 1\n",
    "# precision, recall, thresholds = roc_curve(y_true_binary[:,2], y_pred[:,2]) \n",
    "# # Class 2\n",
    "\n",
    "# # Calculate AUC score for each class\n",
    "# auc_class_0 = auc(recall, precision)\n",
    "# auc_class_1 = auc(recall, precision) \n",
    "# auc_class_2 = auc(recall, precision)\n",
    "\n",
    "# # Take average for overall AUC\n",
    "# auc_score = (auc_class_0 + auc_class_1 + auc_class_2) / 3\n",
    "\n",
    "# # precision, recall, thresholds = roc_curve(y_test_binary, y_pred_proba[:,1])\n",
    "\n",
    "# # auc_score = auc(recall, precision)\n",
    "# ap_score = average_precision_score(y_test_binary, y_pred_proba[:,1])\n",
    "# print(\"PRROC AUC: {}\".format(auc_score))\n",
    "# print(\"PRROC AP: {}\".format(ap_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
